# Real-Time Pose-Based Emote Detection on Raspberry Pi 4

**Team:** VisionMasters  
**Course:** ECE 4332 / ECE 6332 â€” AI Hardware Design and Implementation  
**Platform:** Raspberry Pi 4 Model B (CPU-only inference)

## ğŸ¯ Project Overview

This project implements real-time pose-based gesture recognition that maps body poses to Clash Royale emotes. The system demonstrates edge AI deployment on resource-constrained hardware, measuring performance metrics relevant to AI hardware design.

### Key Features
- **MediaPipe Holistic**: Pre-trained pose detection (33 body landmarks)
- **Feature Engineering**: 45 geometric features extracted from pose landmarks
- **Random Forest Classifier**: Fast, interpretable ML model (<1ms inference)
- **Custom Data Collection**: Train on your own poses
- **Performance Metrics**: Detailed latency, FPS, CPU/memory, and temperature monitoring
- **Real-time Performance**: 10-15 FPS on Raspberry Pi 4 with optimizations

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ src/emote_detector/           # Main application
â”‚   â”œâ”€â”€ main.py                   # Demo application
â”‚   â”œâ”€â”€ data_collector.py         # Collect training data
â”‚   â”œâ”€â”€ train_model.py            # Train & generate evaluation charts
â”‚   â”œâ”€â”€ holistic_detector.py      # MediaPipe wrapper
â”‚   â”œâ”€â”€ pose_classifier.py        # Random Forest classifier
â”‚   â”œâ”€â”€ performance_metrics.py    # Performance profiling
â”‚   â”œâ”€â”€ pose_classifier_model.pkl # Trained model
â”‚   â”œâ”€â”€ pose_data/                # Collected training data
â”‚   â”œâ”€â”€ emotes/                   # Emote images and sounds
â”‚   â””â”€â”€ results/                  # Training results and charts
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ Project_Proposal.md       # Project proposal
â”œâ”€â”€ presentations/                # Presentation slides
â”œâ”€â”€ report/                       # Final report
â””â”€â”€ requirements.txt              # Python dependencies
```

## ğŸš€ Quick Start (Development Machine)

### 1. Install Dependencies

```bash
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

### 2. Collect Training Data

```bash
cd src/emote_detector
python data_collector.py
```

**Controls:**
- `0-4`: Select pose to record
- `SPACE`: Capture single sample
- `a`: Auto-collect samples
- `s`: Save data
- `t`: Train model
- `q`: Quit

### 3. Train Model & Generate Charts

```bash
python train_model.py
```

This generates evaluation charts in `results/charts/`:
- Confusion matrix
- Feature importance
- Per-class accuracy
- Data distribution

### 4. Run Demo

```bash
python main.py
```

## ğŸ“ Raspberry Pi 4 Deployment

### Setup on RPi

```bash
# Install pyenv and Python 3.11 (MediaPipe requires Python <3.12)
curl https://pyenv.run | bash
# Add pyenv to ~/.bashrc, then:
pyenv install 3.11.9
pyenv local 3.11.9

# Create virtual environment
python -m venv venv
source venv/bin/activate

# Install dependencies
pip install numpy opencv-python-headless mediapipe scikit-learn pygame matplotlib seaborn psutil
```

### Copy files to RPi

```bash
scp -r src/emote_detector pi@<RPI_IP>:~/emote_detector/
```

### Run on RPi (via SSH with X11 forwarding)

```bash
ssh -Y pi@<RPI_IP>
cd ~/emote_detector
source venv/bin/activate

# Fast mode (optimized for RPi)
python main.py --fast

# With performance metrics collection
python main.py --fast --metrics
```

### Command Line Options

| Flag | Description |
|------|-------------|
| `--fast` | Optimized mode: 320x240 processing, 640x480 display, skip frames |
| `--metrics` | Enable performance metrics collection |
| `--complexity 0` | Use lightest MediaPipe model |
| `--resolution low` | Process at 320x240 |
| `--skip 2` | Process every 2nd frame |
| `--scale 2` | 2x display scaling |

## ğŸ® Supported Poses (4 Classes)

| ID | Pose | Gesture | Emote |
|----|------|---------|-------|
| 0 | **Laughing** | Hands raised, celebratory | ğŸ˜‚ Laughing King |
| 1 | **Yawning** | Hands near mouth | ğŸ¥± Yawning |
| 2 | **Crying** | Hands covering face | ğŸ˜¢ Crying |
| 3 | **Taunting** | Arms crossed | ğŸ˜ Taunting |

## ğŸ“Š Performance Metrics

### Collected Metrics

| Category | Measurements |
|----------|--------------|
| **Timing** | Frame time, MediaPipe inference, Classifier inference |
| **Throughput** | FPS (mean, min, max) |
| **Latency** | P50, P95, P99 percentiles |
| **System** | CPU %, Memory %, Temperature Â°C |

### Expected Performance (RPi 4)

| Metric | Fast Mode | Standard Mode |
|--------|-----------|---------------|
| **FPS** | 10-15 | 5-8 |
| **Frame Latency** | 70-100ms | 125-200ms |
| **MediaPipe** | 50-80ms | 100-150ms |
| **Classifier** | <1ms | <1ms |
| **CPU Usage** | 70-90% | 80-100% |

### Output Files (with `--metrics`)

```
results/metrics/
â”œâ”€â”€ metrics_TIMESTAMP.json    # Raw data
â”œâ”€â”€ metrics_TIMESTAMP.csv     # For plotting
â””â”€â”€ metrics_TIMESTAMP.md      # Report for presentation
```

## ğŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ USB Webcam  â”‚â”€â”€â”€â”€â–¶â”‚  MediaPipe       â”‚â”€â”€â”€â”€â–¶â”‚  Feature        â”‚â”€â”€â”€â”€â–¶â”‚  Random    â”‚
â”‚ (320x240)   â”‚     â”‚  Holistic        â”‚     â”‚  Extraction     â”‚     â”‚  Forest    â”‚
â”‚             â”‚     â”‚  (TFLite CPU)    â”‚     â”‚  (45 features)  â”‚     â”‚  Classifierâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚                    â”‚                        â”‚                      â”‚
      â”‚              33 Pose                   Geometric              Pose Label +
      â”‚              Landmarks                 Features               Confidence
      â–¼                    â–¼                        â–¼                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Display (640x480) + Emote Overlay + Sound                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”¬ AI Hardware Insights

This project demonstrates key AI hardware concepts:

1. **Model Selection Trade-offs**: MediaPipe (TFLite) enables real-time pose detection on CPU
2. **Feature Engineering vs. Deep Learning**: Geometric features + Random Forest is more efficient than end-to-end CNN on CPU
3. **Pipeline Optimization**: Resolution reduction, frame skipping, and display scaling improve throughput
4. **Resource Profiling**: Metrics collection reveals bottlenecks (MediaPipe dominates inference time)
5. **Thermal Management**: Sustained operation requires performance trade-offs to avoid throttling

## ğŸ‘¥ Team Members

- **Allen Chen** (wmm7wr@virginia.edu) - Hardware Integration
- **Marvin Rivera** (tkk9wg@virginia.edu) - Team Lead, Documentation
- **Sami Kang** (ajp3cx@virginia.edu) - Model Training, Inference

## ğŸ“š References

- [MediaPipe Holistic](https://google.github.io/mediapipe/solutions/holistic.html)
- [scikit-learn Random Forest](https://scikit-learn.org/stable/modules/ensemble.html#forests-of-randomized-trees)
- [Raspberry Pi 4 Documentation](https://www.raspberrypi.com/documentation/)
- [TensorFlow Lite](https://www.tensorflow.org/lite)

## ğŸ“ License

This project is for educational purposes as part of ECE 4332/6332 at the University of Virginia.
